{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\n",
    "from keras.initializers import Constant\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.tsv', sep='\\t')\n",
    "df_test=pd.read_csv('test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['Phrase', 'Sentiment']]\n",
    "\n",
    "# Set your model output as categorical and save in new label col\n",
    "data['Sentiment_label'] = pd.Categorical(data['Sentiment'])\n",
    "\n",
    "# Transform your output to numeric\n",
    "data['Sentiment'] = data['Sentiment_label'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = train_test_split(data, test_size = 0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel,  BertConfig, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Name of the BERT model to use\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Max length of tokens\n",
    "max_length = 45\n",
    "\n",
    "# Load transformers config and set output_hidden_states to False\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n",
    "\n",
    "# Load the Transformers BERT model\n",
    "transformer_bert_model = TFBertModel.from_pretrained(model_name, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiClass\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_ids (InputLayer)      [(None, 45)]              0         \n",
      "                                                                 \n",
      " bert (TFBertMainLayer)      TFBaseModelOutputWithPoo  109482240 \n",
      "                             lingAndCrossAttentions(l            \n",
      "                             ast_hidden_state=(None,             \n",
      "                             45, 768),                           \n",
      "                              pooler_output=(None, 76            \n",
      "                             8),                                 \n",
      "                              past_key_values=None, h            \n",
      "                             idden_states=None, atten            \n",
      "                             tions=None, cross_attent            \n",
      "                             ions=None)                          \n",
      "                                                                 \n",
      " pooled_output (Dropout)     (None, 768)               0         \n",
      "                                                                 \n",
      " Sentiment (Dense)           (None, 5)                 3845      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 109,486,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### ------- Build the model ------- ###\n",
    "\n",
    "# Load the MainLayer\n",
    "bert = transformer_bert_model.layers[0]\n",
    "\n",
    "# Build your model input\n",
    "input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "inputs = {'input_ids': input_ids}\n",
    "\n",
    "# Load the Transformers BERT model as a layer in a Keras model\n",
    "bert_model = bert(inputs)[1]\n",
    "dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
    "pooled_output = dropout(bert_model, training=False)\n",
    "\n",
    "# Then build your model output\n",
    "Sentiments = Dense(units=len(data_train.Sentiment_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='Sentiment')(pooled_output)\n",
    "outputs = {'Sentiment': Sentiments}\n",
    "\n",
    "# And combine it all in a model object\n",
    "model = Model(inputs=inputs, outputs=outputs, name='BERT_MultiClass')\n",
    "\n",
    "# Take a look at the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1415/1415 [==============================] - 24355s 17s/step - loss: 0.8412 - accuracy: 0.6526 - val_loss: 0.7607 - val_accuracy: 0.6818\n",
      "Epoch 2/2\n",
      "1415/1415 [==============================] - 18315s 13s/step - loss: 0.7137 - accuracy: 0.7051 - val_loss: 0.7442 - val_accuracy: 0.6889\n"
     ]
    }
   ],
   "source": [
    "### ------- Train the model ------- ###\n",
    "\n",
    "# Set an optimizer\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n",
    "\n",
    "# Set loss and metrics\n",
    "loss = {'Sentiment': CategoricalCrossentropy(from_logits = True)}\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
    "\n",
    "# Ready output data for the model\n",
    "y_train = to_categorical(data_train['Sentiment'])\n",
    "\n",
    "# Tokenize the input (takes some time)\n",
    "x_train = tokenizer(\n",
    "          text=data_train['Phrase'].to_list(),\n",
    "          add_special_tokens=True,\n",
    "          max_length=max_length,\n",
    "          truncation=True,\n",
    "          padding=True, \n",
    "          return_tensors='tf',\n",
    "          return_token_type_ids = False,\n",
    "          return_attention_mask = True,\n",
    "          verbose = True)\n",
    "\n",
    "y_val = to_categorical(data_val['Sentiment'])\n",
    "\n",
    "x_val = tokenizer(\n",
    "          text=data_val['Phrase'].to_list(),\n",
    "          add_special_tokens=True,\n",
    "          max_length=max_length,\n",
    "          truncation=True,\n",
    "          padding=True, \n",
    "          return_tensors='tf',\n",
    "          return_token_type_ids = False,\n",
    "          return_attention_mask = True,\n",
    "          verbose = True)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x={'input_ids': x_train['input_ids']},\n",
    "    y={'Sentiment': y_train},\n",
    "    validation_data=({'input_ids': x_val['input_ids']},{'Sentiment': y_val}),\n",
    "    batch_size=64,\n",
    "    epochs=2,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
